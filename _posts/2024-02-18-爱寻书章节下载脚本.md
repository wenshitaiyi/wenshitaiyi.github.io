---
layout: post
title: 2024-02-18-爱寻书章节下载脚本
subtitle: 真是没有想到，这个网站竟然可以下载知乎盐选的部分文章
aliases: 
date: 2024-02-18 22:28:33
author: 雯饰太一
header-img: img/post-bg-default.jpeg
tags:
  - 代码片段
  - js
typora-copy-images-to: ../img/post
typora-root-url: ../
---
	网站的地址如下：[寻书找书就来爱寻书阅读网-爱寻书](https://ixunshu.net/)

> 希望这个网站可以多存活一段时间，让我能下载更多的小说

下面的脚本都是直接通过AI生成的，只需要描述清楚每一个函数是用来干什么的，自己去整合下的逻辑即可。

下载的流程主要分成三个部分：
- 第一个获取当前页面的文章内容
- 第二个请求指定的链接获取HTML源码
- 第三个当前章节有多少个页面循环遍历，将所有的内容拼接在一起

```js
function getContentFromHTML(html) {
    // 创建一个临时 div 元素用于解析 HTML
    var tempDiv = document.createElement('div');
    tempDiv.innerHTML = html;

    // 获取 <div id="booktxt"> 元素
    var booktxtDiv = tempDiv.querySelector('#booktxt');

    // 如果未找到对应的 div，则提示错误并返回空字符串
    if (!booktxtDiv) {
        console.error('未找到 id 为 "booktxt" 的 div 元素');
        return '';
    }

    // 获取所有 <p> 元素
    var pElements = booktxtDiv.querySelectorAll('p');

    // 初始化一个空字符串来保存内容
    var combinedText = '';

    // 遍历所有 <p> 元素，将其内容添加到 combinedText 中
    pElements.forEach(function(pElement) {
        combinedText += pElement.textContent.trim() + '\n'; // 使用 trim() 来移除空白
    });

    // 返回合并后的文本
    return combinedText;
}


// 定义获取 HTML 源码并提取内容的函数
async function getContentFromURL(url) {
    try {
        // 使用 Fetch 请求获取 HTML 源码
        const response = await fetch(url);

        // 检查响应状态
        if (!response.ok) {
            throw new Error('网络请求出错：' + response.status);
        }

        // 提取响应的文本内容
        const html = await response.text();

        // 调用之前定义的函数来获取内容
        const content = getContentFromHTML(html);

        // 返回提取的内容
        return content;
    } catch (error) {
        console.error('获取内容时出错：', error);
        return ''; // 如果出现错误，返回空字符串
    }
}


// 初始化一个空字符串来保存所有内容
let allContent = '';

// 定义基础 URL 和页数范围
const baseURL = 'https://ixunshu.net/xs/18360426/zj/199638458?page=';
const startPage = 1;
const endPage = 32;

// 使用 for 循环访问每个链接并提取内容
for (let i = startPage; i <= endPage; i++) {
    const url = baseURL + i;
    const content = await getContentFromURL(url);
    console.log('get url: '+url+'\n'+ content);
    allContent += content + '\n'; // 将内容拼接到 allContent 中
}

// 输出所有内容
console.log(allContent);

```

上面的脚本也是有弊端的，它仅能获取当前这个章节，并不能根据指定的小说页面获取这本小说的内容。

后续可以将这部分下载的脚本全都整合到油猴插件之中，根据不同的小说网站适配不同的下载规则，在客户端直接下载的这种形式应该摒弃掉，他有点儿笨拙，而且不容易修改。每次发现了新的小说网站之后，改起来就非常复杂。虽然有一个专门的配置文件用来定义不同的网站的下载规则。但是时间长了也会忘记，倒不如针对某一个小说网站直接写死一个脚本。如果还有新的网站的话，就直接进行复制粘贴，毕竟在Chrome浏览器之中调试还是很方便的。666。